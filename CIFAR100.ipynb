{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import sys\n",
    "if sys.version_info[0] == 2:\n",
    "    import cPickle as pickle\n",
    "else:\n",
    "    import pickle\n",
    "\n",
    "import torch.utils.data as data\n",
    "#from torch.utils import download_url, check_integrity\n",
    "\n",
    "import pdb\n",
    "\n",
    "\n",
    "class CIFAR10(data.Dataset):\n",
    "    \"\"\"`CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where directory\n",
    "            ``cifar-10-batches-py`` exists or will be saved to if download is set to True.\n",
    "        train (bool, optional): If True, creates dataset from training set, otherwise\n",
    "            creates from test set.\n",
    "        transform (callable, optional): A function/transform that takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "    \"\"\"\n",
    "    base_folder = 'cifar-10-batches-py'\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    filename = \"cifar-10-python.tar.gz\"\n",
    "    tgz_md5 = 'c58f30108f718f92721af3b95e74349a'\n",
    "    train_list = [\n",
    "        ['data_batch_1', 'c99cafc152244af753f735de768cd75f'],\n",
    "        ['data_batch_2', 'd4bba439e000b95fd0a9bffe97cbabec'],\n",
    "        ['data_batch_3', '54ebc095f3ab1f0389bbae665268c751'],\n",
    "        ['data_batch_4', '634d18415352ddfa80567beed471001a'],\n",
    "        ['data_batch_5', '482c414d41f54cd18b22e5b47cb7c3cb'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test_batch', '40351d587109b95175f43aff81a1287e'],\n",
    "    ]\n",
    "    meta = {\n",
    "        'filename': 'batches.meta',\n",
    "        'key': 'label_names',\n",
    "        'md5': '5ff9c542aee3614f3951f8cda6e48888',\n",
    "    }\n",
    "\n",
    "    def __init__(self, root, train=True,\n",
    "                 transform=None, target_transform=None,\n",
    "                 download=False, index=None,num_instance_per_class=0):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train  # training set or test set\n",
    "\n",
    "        # if download:\n",
    "        #     self.download()\n",
    "\n",
    "        # if not self._check_integrity():\n",
    "        #     raise RuntimeError('Dataset not found or corrupted.' +\n",
    "        #                        ' You can use download=True to download it')\n",
    "\n",
    "        if self.train:\n",
    "            downloaded_list = self.train_list\n",
    "        else:\n",
    "            downloaded_list = self.test_list\n",
    "\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "\n",
    "        # now load the picked numpy arrays\n",
    "        for file_name, checksum in downloaded_list:\n",
    "            file_path = os.path.join(self.root, self.base_folder, file_name)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                if sys.version_info[0] == 2:\n",
    "                    entry = pickle.load(f)\n",
    "                else:\n",
    "                    entry = pickle.load(f, encoding='latin1')\n",
    "                self.data.append(entry['data'])\n",
    "                if 'labels' in entry:\n",
    "                    self.targets.extend(entry['labels'])\n",
    "                else:\n",
    "                    self.targets.extend(entry['fine_labels'])\n",
    "\n",
    "        self.data = np.vstack(self.data).reshape(-1, 3, 32, 32)\n",
    "        self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC\n",
    "        #self.data = self.data/255.\n",
    "        #pdb.set_trace()\n",
    "        self.targets = np.asarray(self.targets)\n",
    "        #index_sort = np.argsort(self.targets)\n",
    "        # Sort label and corresponding data from 0-9\n",
    "        #self.data = self.data[index_sort]\n",
    "        #self.targets=np.asarray(sorted(self.targets))\n",
    "        #pdb.set_trace()\n",
    "        if num_instance_per_class==0:\n",
    "            self.data,self.targets = self.RandomPercentage(self.data,self.targets,index)\n",
    "        else:\n",
    "            self.data,self.targets = self.RandomExempalers(self.data,self.targets,index,num_instance_per_class)\n",
    "\n",
    "        self._load_meta()\n",
    "\n",
    "    def RandomPercentage(self, data,targets,index):\n",
    "        data_tmp = []\n",
    "        targets_tmp = []\n",
    "        for i in index:\n",
    "            ind_cl = np.where(i == targets)[0]\n",
    "            if data_tmp==[]:\n",
    "                data_tmp = data[ind_cl]\n",
    "                targets_tmp = targets[ind_cl]\n",
    "            else:\n",
    "                data_tmp = np.vstack((data_tmp,data[ind_cl]))\n",
    "                targets_tmp = np.hstack((targets_tmp,targets[ind_cl]))\n",
    "\n",
    "        return data_tmp,targets_tmp\n",
    "    def RandomExempalers(self, data,targets,index,num):\n",
    "        data_tmp = []\n",
    "        targets_tmp = []\n",
    "        for i in index:\n",
    "            ind_cl = np.where(i == targets)[0][:num]\n",
    "            if data_tmp==[]:\n",
    "                data_tmp = data[ind_cl]\n",
    "                targets_tmp = targets[ind_cl]\n",
    "            else:\n",
    "                data_tmp = np.vstack((data_tmp,data[ind_cl]))\n",
    "                targets_tmp = np.hstack((targets_tmp,targets[ind_cl]))\n",
    "                \n",
    "        return data_tmp,targets_tmp\n",
    "    def _load_meta(self):\n",
    "        path = os.path.join(self.root, self.base_folder, self.meta['filename'])\n",
    "        # if not check_integrity(path, self.meta['md5']):\n",
    "        #     raise RuntimeError('Dataset metadata file not found or corrupted.' +\n",
    "        #                        ' You can use download=True to download it')\n",
    "        with open(path, 'rb') as infile:\n",
    "            if sys.version_info[0] == 2:\n",
    "                data = pickle.load(infile)\n",
    "            else:\n",
    "                data = pickle.load(infile, encoding='latin1')\n",
    "            self.classes = data[self.meta['key']]\n",
    "        self.class_to_idx = {_class: i for i, _class in enumerate(self.classes)}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # def _check_integrity(self):\n",
    "    #     root = self.root\n",
    "    #     for fentry in (self.train_list + self.test_list):\n",
    "    #         filename, md5 = fentry[0], fentry[1]\n",
    "    #         fpath = os.path.join(root, self.base_folder, filename)\n",
    "    #         if not check_integrity(fpath, md5):\n",
    "    #             return False\n",
    "    #     return True\n",
    "\n",
    "    # def download(self):\n",
    "    #     import tarfile\n",
    "\n",
    "    #     # if self._check_integrity():\n",
    "    #     #     print('Files already downloaded and verified')\n",
    "    #     #     return\n",
    "\n",
    "    #     download_url(self.url, self.root, self.filename, self.tgz_md5)\n",
    "\n",
    "    #     # extract file\n",
    "    #     with tarfile.open(os.path.join(self.root, self.filename), \"r:gz\") as tar:\n",
    "    #         tar.extractall(path=self.root)\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        tmp = 'train' if self.train is True else 'test'\n",
    "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        tmp = '    Transforms (if any): '\n",
    "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        tmp = '    Target Transforms (if any): '\n",
    "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        return fmt_str\n",
    "\n",
    "class CIFAR100(CIFAR10):\n",
    "    \"\"\"`CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
    "    This is a subclass of the `CIFAR10` Dataset.\n",
    "    \"\"\"\n",
    "    base_folder = 'cifar-100-python'\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
    "    filename = \"cifar-100-python.tar.gz\"\n",
    "    tgz_md5 = 'eb9058c3a382ffc7106e4002c42a8d85'\n",
    "    train_list = [\n",
    "        ['train', '16019d7e3df5f24257cddd939b257f8d'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test', 'f0ef6b0ae62326f3e7ffdfab6717acfc'],\n",
    "    ]\n",
    "    meta = {\n",
    "        'filename': 'meta',\n",
    "        'key': 'fine_label_names',\n",
    "        'md5': '7973b15100ade9c7d40fb424638fde48',\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
