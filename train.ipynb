{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "from __future__ import absolute_import, print_function\n",
    "from copy import deepcopy\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import torch.utils.data\n",
    "from torch.backends import cudnn\n",
    "from torch.autograd import Variable\n",
    "import models\n",
    "import losses\n",
    "from utils import RandomIdentitySampler, mkdir_if_missing, logging, display\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import pdb\n",
    "import numpy as np\n",
    "from ImageFolder import *\n",
    "import torchvision.transforms as transforms\n",
    "from evaluations import extract_features, pairwise_distance\n",
    "from CIFAR100 import CIFAR100\n",
    "\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def get_model(model):\n",
    "    return deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "def set_model_(model, state_dict):\n",
    "    model.load_state_dict(deepcopy(state_dict))\n",
    "    return model\n",
    "\n",
    "\n",
    "def freeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model\n",
    "\n",
    "\n",
    "def fisher_matrix_diag(model, criterion, train_loader, number_samples=500):\n",
    "\n",
    "    # Init\n",
    "    fisher = {}\n",
    "    for n, p in model.named_parameters():\n",
    "        fisher[n] = 0*p.data\n",
    "\n",
    "    model.train()\n",
    "    count = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        count += 1\n",
    "        inputs, labels = data\n",
    "        # wrap them in Variable\n",
    "        inputs = Variable(inputs.cuda())\n",
    "        labels = Variable(labels).cuda()\n",
    "\n",
    "        # Forward and backward\n",
    "        model.zero_grad()\n",
    "        if args.method == 'MAS':\n",
    "            embed_feat = model.forward_without_norm(inputs)\n",
    "            loss = torch.sum(torch.norm(embed_feat, 2, dim=1))\n",
    "        elif args.method == 'EWC':\n",
    "            _, embed_feat = model(inputs)\n",
    "            if args.loss == 'MSLoss':\n",
    "                loss = criterion(embed_feat, labels)\n",
    "            else:\n",
    "                loss, _, _, _ = criterion(embed_feat, labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.grad is not None:\n",
    "                fisher[n] += p.grad.data.pow(2)\n",
    "\n",
    "    for n, _ in model.named_parameters():\n",
    "        fisher[n] = fisher[n]/float(count)\n",
    "        fisher[n] = torch.autograd.Variable(fisher[n], requires_grad=False)\n",
    "    return fisher\n",
    "\n",
    "\n",
    "def compute_prototype(model, data_loader):\n",
    "\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    embeddings = []\n",
    "    embeddings_labels = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(data_loader, 0):\n",
    "            count += 1\n",
    "            inputs, labels = data\n",
    "            # wrap them in Variable\n",
    "            inputs = Variable(inputs.cuda())\n",
    "            _, embed_feat = model(inputs)\n",
    "            embeddings_labels.append(labels.numpy())\n",
    "            embeddings.append(embed_feat.cpu().numpy())\n",
    "\n",
    "    embeddings = np.asarray(embeddings)\n",
    "    embeddings = np.reshape(\n",
    "        embeddings, (embeddings.shape[0]*embeddings.shape[1], embeddings.shape[2]))\n",
    "    embeddings_labels = np.asarray(embeddings_labels)\n",
    "    embeddings_labels = np.reshape(\n",
    "        embeddings_labels, embeddings_labels.shape[0]*embeddings_labels.shape[1])\n",
    "    labels_set = np.unique(embeddings_labels)\n",
    "    class_mean = []\n",
    "    class_std = []\n",
    "    class_label = []\n",
    "    for i in labels_set:\n",
    "        ind_cl = np.where(i == embeddings_labels)[0]\n",
    "        embeddings_tmp = embeddings[ind_cl]\n",
    "        class_label.append(i)\n",
    "        class_mean.append(np.mean(embeddings_tmp, axis=0))\n",
    "        class_std.append(np.std(embeddings_tmp, axis=0))\n",
    "    prototype = {'class_mean_old': class_mean, 'class_mean': class_mean,\n",
    "                 'class_std': class_std, 'class_label': class_label}\n",
    "\n",
    "    return prototype\n",
    "\n",
    "\n",
    "def train_fun(args, train_loader, feat_loader, current_task, fisher={}, prototype={}):\n",
    "\n",
    "    log_dir = os.path.join('checkpoints', args.log_dir)\n",
    "    mkdir_if_missing(log_dir)\n",
    "\n",
    "    sys.stdout = logging.Logger(os.path.join(log_dir, 'log.txt'))\n",
    "    display(args)\n",
    "\n",
    "    model = models.create(args.net, Embed_dim=args.dim)\n",
    "    # load part of the model\n",
    "    if args.method == 'Independent' or current_task == 0:\n",
    "        model_dict = model.state_dict()\n",
    "\n",
    "        if args.net == 'resnet32':\n",
    "            if args.base == 50:\n",
    "                pretrained_dict = torch.load(\n",
    "                    'pretrained_models/Finetuning_0_task_0_200_model_task2_cifar100_seed1993.pkl')\n",
    "            pretrained_dict = pretrained_dict.state_dict()\n",
    "            pretrained_dict = {k: v for k, v in pretrained_dict.items(\n",
    "            ) if k in model_dict and 'fc' not in k}\n",
    "            model_dict.update(pretrained_dict)\n",
    "            model.load_state_dict(model_dict)\n",
    "\n",
    "        elif args.net == 'resnet18' and args.data == 'imagenet_sub':\n",
    "            if args.base == 50:\n",
    "                pretrained_dict = torch.load(\n",
    "                    'pretrained_models/Finetuning_0_task_0_200_model_task2_imagenet_sub_seed1993.pkl')\n",
    "            pretrained_dict = pretrained_dict.state_dict()\n",
    "            pretrained_dict = {k: v for k, v in pretrained_dict.items(\n",
    "            ) if k in model_dict and 'fc' not in k}\n",
    "            model_dict.update(pretrained_dict)\n",
    "            model.load_state_dict(model_dict)\n",
    "\n",
    "        else:\n",
    "            print (' Oops!  That was no valid models. ')\n",
    "\n",
    "    if args.method != 'Independent' and current_task > 0:\n",
    "        model = torch.load(os.path.join(log_dir, args.method + '_' + args.exp +\n",
    "                                        '_task_' + str(current_task-1) + '_%d_model.pkl' % int(args.epochs-1)))\n",
    "        model_old = deepcopy(model)\n",
    "        model_old.eval()\n",
    "        model_old = freeze_model(model_old)\n",
    "\n",
    "    model = model.cuda()\n",
    "    torch.save(model, os.path.join(log_dir, args.method + '_' +\n",
    "                                   args.exp + '_task_' + str(current_task) + '_pre_model.pkl'))\n",
    "    print('initial model is save at %s' % log_dir)\n",
    "\n",
    "    # fine tune the model: the learning rate for pre-trained parameter is 1/10\n",
    "    new_param_ids = set(map(id, model.Embed.parameters()))\n",
    "\n",
    "    new_params = [p for p in model.parameters() if\n",
    "                  id(p) in new_param_ids]\n",
    "\n",
    "    base_params = [p for p in model.parameters() if\n",
    "                   id(p) not in new_param_ids]\n",
    "    param_groups = [\n",
    "        {'params': base_params, 'lr_mult': 0.1},\n",
    "        {'params': new_params, 'lr_mult': 1.0}]\n",
    "\n",
    "    criterion = losses.create(args.loss, margin=args.margin, num_instances=args.num_instances).cuda()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        param_groups, lr=args.lr, weight_decay=args.weight_decay)\n",
    "    scheduler = StepLR(optimizer, step_size=200, gamma=0.1)\n",
    "\n",
    "    if args.data == 'cifar100' or args.data == 'imagenet_sub':\n",
    "        if current_task > 0:\n",
    "            model.eval()\n",
    "\n",
    "    for epoch in range(args.start, args.epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_lwf = 0.0\n",
    "        scheduler.step()\n",
    "\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            # wrap them in Variable\n",
    "            inputs = Variable(inputs.cuda())\n",
    "            labels = Variable(labels).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            _, embed_feat = model(inputs)\n",
    "\n",
    "            if current_task == 0:\n",
    "                loss_aug = 0*torch.sum(embed_feat)\n",
    "            else:\n",
    "                if args.method == 'Finetuning' or args.method == 'Independent':\n",
    "                    loss_aug = 0*torch.sum(embed_feat)\n",
    "                elif args.method == 'LwF':\n",
    "                    _, embed_feat_old = model_old(inputs)\n",
    "                    loss_aug = args.tradeoff * \\\n",
    "                        torch.sum((embed_feat-embed_feat_old).pow(2))/2.\n",
    "                elif args.method == 'EWC' or args.method == 'MAS':\n",
    "                    loss_aug = 0\n",
    "                    for (name, param), (_, param_old) in zip(model.named_parameters(), model_old.named_parameters()):\n",
    "                        loss_aug += args.tradeoff * \\\n",
    "                            torch.sum(fisher[name]*(param_old-param).pow(2))/2.\n",
    "\n",
    "            embed_sythesis = []\n",
    "            embed_label_sythesis = []\n",
    "\n",
    "            if args.loss == 'MSLoss':\n",
    "                loss = criterion(embed_feat, labels)\n",
    "                inter_ = 0\n",
    "                dist_ap = 0\n",
    "                dist_an = 0\n",
    "            else:\n",
    "                loss, inter_, dist_ap, dist_an = criterion(embed_feat, labels)\n",
    "            loss += loss_aug\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.data[0]\n",
    "            running_lwf += loss_aug.data[0]\n",
    "            if epoch == 0 and i == 0:\n",
    "                print(50*'#')\n",
    "                print('Train Begin -- HA-HA-HA')\n",
    "\n",
    "        print('[Epoch %05d]\\t Total Loss: %.3f \\t LwF Loss: %.3f \\t Accuracy: %.3f \\t Pos-Dist: %.3f \\t Neg-Dist: %.3f'\n",
    "              % (epoch + 1,  running_loss, running_lwf, inter_, dist_ap, dist_an))\n",
    "\n",
    "        if epoch % args.save_step == 0:\n",
    "            torch.save(model, os.path.join(log_dir, args.method + '_' +\n",
    "                                           args.exp + '_task_' + str(current_task) + '_%d_model.pkl' % epoch))\n",
    "\n",
    "    if args.method == 'EWC' or args.method == 'MAS':\n",
    "        fisher = fisher_matrix_diag(\n",
    "            model, criterion, train_loader, number_samples=500)\n",
    "        return fisher\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='KNN-Softmax Training')\n",
    "\n",
    "    # hype-parameters\n",
    "    parser.add_argument('-lr', type=float, default=1e-4,\n",
    "                        help=\"learning rate of new parameters\")\n",
    "    parser.add_argument('-tradeoff', type=float, default=1.0,\n",
    "                        help=\"learning rate of new parameters\")\n",
    "    parser.add_argument('-exp', type=str, default='exp1',\n",
    "                        help=\"learning rate of new parameters\")\n",
    "    parser.add_argument('-margin', type=float, default=0.0,\n",
    "                        help=\"margin for metric loss\")\n",
    "\n",
    "    parser.add_argument('-BatchSize', '-b', default=128, type=int, metavar='N',\n",
    "                        help='mini-batch size (1 = pure stochastic) Default: 256')\n",
    "    parser.add_argument('-num_instances', default=8, type=int, metavar='n',\n",
    "                        help=' number of samples from one class in mini-batch')\n",
    "    parser.add_argument('-dim', default=512, type=int, metavar='n',\n",
    "                        help='dimension of embedding space')\n",
    "    parser.add_argument('-alpha', default=30, type=int, metavar='n',\n",
    "                        help='hyper parameter in KNN Softmax')\n",
    "    parser.add_argument('-k', default=16, type=int, metavar='n',\n",
    "                        help='number of neighbour points in KNN')\n",
    "\n",
    "    # network\n",
    "    parser.add_argument('-data', default='cub', required=True,\n",
    "                        help='path to Data Set')\n",
    "    parser.add_argument('-net', default='bn')\n",
    "    parser.add_argument('-loss', default='branch', required=True,\n",
    "                        help='loss for training network')\n",
    "    parser.add_argument('-epochs', default=200, type=int, metavar='N',\n",
    "                        help='epochs for training process')\n",
    "\n",
    "    parser.add_argument('-seed', default=1993, type=int, metavar='N',\n",
    "                        help='seeds for training process')\n",
    "    parser.add_argument('-save_step', default=50, type=int, metavar='N',\n",
    "                        help='number of epochs to save model')\n",
    "    parser.add_argument('-lr_step', default=200, type=int, metavar='N',\n",
    "                        help='number of epochs to save model')\n",
    "    # Resume from checkpoint\n",
    "    parser.add_argument('-start', default=0, type=int,\n",
    "                        help='resume epoch')\n",
    "\n",
    "    # basic parameter\n",
    "    parser.add_argument('-log_dir', default=None,\n",
    "                        help='where the trained models save')\n",
    "    parser.add_argument('--nThreads', '-j', default=2, type=int, metavar='N',\n",
    "                        help='number of data loading threads (default: 2)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9)\n",
    "    parser.add_argument('--weight-decay', type=float, default=2e-4)\n",
    "    parser.add_argument(\"-gpu\", type=str, default='0',\n",
    "                        help='which gpu to choose')\n",
    "    parser.add_argument(\"-method\", type=str,\n",
    "                        default='Finetuning', help='Choose FT or SC')\n",
    "\n",
    "    parser.add_argument('-mapping_mean', default='no',\n",
    "                        type=str, help='mapping')\n",
    "    parser.add_argument('-sigma', default=0.0, type=float, help='sigma')\n",
    "    parser.add_argument('-vez', default=0, type=int, help='vez')\n",
    "    parser.add_argument('-task', default=0, type=int, help='vez')\n",
    "    parser.add_argument('-base', default=50, type=int, help='vez')\n",
    "\n",
    "    # parser.add_argument('-evel', action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    # Data\n",
    "    print('==> Preparing data..')\n",
    "\n",
    "    if args.data == \"cifar100\":\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                 (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        root = 'DataSet'\n",
    "        traindir = root + '/cifar'\n",
    "        num_classes = 100\n",
    "\n",
    "    if args.data == 'cub':\n",
    "        mean_values = [0.485, 0.456, 0.406]\n",
    "        std_values = [0.229, 0.224, 0.225]\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean_values,\n",
    "                                 std=std_values)\n",
    "        ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean_values,\n",
    "                                 std=std_values),\n",
    "        ])\n",
    "        root = 'DataSet/CUB_200_2011'\n",
    "        traindir = os.path.join(root, 'train')\n",
    "        testdir = os.path.join(root, 'test')\n",
    "\n",
    "        num_classes = 200\n",
    "\n",
    "    if args.data == 'car':\n",
    "        mean_values = [0.485, 0.456, 0.406]\n",
    "        std_values = [0.229, 0.224, 0.225]\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean_values,\n",
    "                                 std=std_values)\n",
    "        ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean_values,\n",
    "                                 std=std_values),\n",
    "        ])\n",
    "        root = 'DataSet/Car196'\n",
    "        traindir = os.path.join(root, 'train')\n",
    "        testdir = os.path.join(root, 'test')\n",
    "\n",
    "        num_classes = 196\n",
    "\n",
    "    if args.data == 'flower':\n",
    "        mean_values = [0.485, 0.456, 0.406]\n",
    "        std_values = [0.229, 0.224, 0.225]\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean_values,\n",
    "                                 std=std_values)\n",
    "        ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean_values,\n",
    "                                 std=std_values),\n",
    "        ])\n",
    "        root = 'DataSet/flowers'\n",
    "        traindir = os.path.join(root, 'train')\n",
    "        testdir = os.path.join(root, 'test')\n",
    "\n",
    "        num_classes = 102\n",
    "\n",
    "    if args.data == 'imagenet_sub' or args.data == 'imagenet_full':\n",
    "        mean_values = [0.485, 0.456, 0.406]\n",
    "        std_values = [0.229, 0.224, 0.225]\n",
    "        transform_train = transforms.Compose([\n",
    "            # transforms.Resize(256),\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean_values,\n",
    "                                 std=std_values)\n",
    "        ])\n",
    "        root = '/datatmp/datasets/ILSVRC12_256'\n",
    "        traindir = os.path.join(root, 'train')\n",
    "        num_classes = 100\n",
    "\n",
    "    num_task = args.task\n",
    "    num_class_per_task = (num_classes-args.base)/(num_task-1)\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    random_perm = np.random.permutation(num_classes)\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "\n",
    "    fisher = {}\n",
    "    prototype = {}\n",
    "    for i in range(num_task):\n",
    "        if i == 0:\n",
    "            class_index = random_perm[:args.base]\n",
    "        else:\n",
    "            class_index = random_perm[args.base +\n",
    "                                      (i-1)*num_class_per_task:args.base+i*num_class_per_task]\n",
    "        if args.data == 'cifar100':\n",
    "            trainfolder = CIFAR100(\n",
    "                root=traindir, train=True, download=True, transform=transform_train, index=class_index)\n",
    "        else:\n",
    "            trainfolder = ImageFolder(\n",
    "                traindir, transform_train, index=class_index)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            trainfolder, batch_size=args.BatchSize,\n",
    "            sampler=RandomIdentitySampler(\n",
    "                trainfolder, num_instances=args.num_instances),\n",
    "            drop_last=True, num_workers=args.nThreads)\n",
    "\n",
    "        feat_loader = torch.utils.data.DataLoader(\n",
    "            trainfolder, batch_size=1, shuffle=True, drop_last=False)\n",
    "        # Fix the random seed to be sure we have the same permutation for one experiment\n",
    "        if args.method == 'EWC' or args.method == 'MAS':\n",
    "            fisher = train_fun(args, train_loader,\n",
    "                               feat_loader, i, fisher=fisher)\n",
    "        else:\n",
    "            train_fun(args, train_loader, feat_loader, i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
