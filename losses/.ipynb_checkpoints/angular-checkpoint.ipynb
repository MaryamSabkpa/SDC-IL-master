{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Constants\n",
    "N_PAIR = 'n-pair'\n",
    "ANGULAR = 'angular'\n",
    "N_PAIR_ANGULAR = 'n-pair-angular'\n",
    "MAIN_LOSS_CHOICES = (N_PAIR, ANGULAR, N_PAIR_ANGULAR)\n",
    "\n",
    "CROSS_ENTROPY = 'cross-entropy'\n",
    "\n",
    "\n",
    "class BlendedLoss(object):\n",
    "    def __init__(self, main_loss_type, cross_entropy_flag):\n",
    "        super(BlendedLoss, self).__init__()\n",
    "        self.main_loss_type = main_loss_type\n",
    "        assert main_loss_type in MAIN_LOSS_CHOICES, \"invalid main loss: %s\" % main_loss_type\n",
    "\n",
    "        self.metrics = []\n",
    "        if self.main_loss_type == N_PAIR:\n",
    "            self.main_loss_fn = NPairLoss()\n",
    "        elif self.main_loss_type == ANGULAR:\n",
    "            self.main_loss_fn = AngularLoss()\n",
    "        elif self.main_loss_type == N_PAIR_ANGULAR:\n",
    "            self.main_loss_fn = NPairAngularLoss()\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        self.cross_entropy_flag = cross_entropy_flag\n",
    "        self.lambda_blending = 0\n",
    "        if cross_entropy_flag:\n",
    "            self.cross_entropy_loss_fn = nn.CrossEntropyLoss()\n",
    "            self.lambda_blending = 0.3\n",
    "\n",
    "    def calculate_loss(self, target, output_embedding, output_cross_entropy=None):\n",
    "        if target is not None:\n",
    "            target = (target, )\n",
    "\n",
    "        loss_dict = {}\n",
    "        blended_loss = 0\n",
    "        if self.cross_entropy_flag:\n",
    "            assert output_cross_entropy is not None, \"Outputs for cross entropy loss is needed\"\n",
    "\n",
    "            loss_inputs = self._gen_loss_inputs(target, output_cross_entropy)\n",
    "            cross_entropy_loss = self.cross_entropy_loss_fn(*loss_inputs)\n",
    "            blended_loss += self.lambda_blending * cross_entropy_loss\n",
    "            loss_dict[CROSS_ENTROPY + '-loss'] = [cross_entropy_loss.item()]\n",
    "\n",
    "        loss_inputs = self._gen_loss_inputs(target, output_embedding)\n",
    "        main_loss_outputs = self.main_loss_fn(*loss_inputs)\n",
    "        main_loss = main_loss_outputs[0] if type(main_loss_outputs) in (tuple, list) else main_loss_outputs\n",
    "        blended_loss += (1 - self.lambda_blending) * main_loss\n",
    "        loss_dict[self.main_loss_type + '-loss'] = [main_loss.item()]\n",
    "\n",
    "        for metric in self.metrics:\n",
    "            metric(output_embedding, target, main_loss_outputs)\n",
    "\n",
    "        return blended_loss, loss_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def _gen_loss_inputs(target, embedding):\n",
    "        if type(embedding) not in (tuple, list):\n",
    "            embedding = (embedding, )\n",
    "        loss_inputs = embedding\n",
    "        if target is not None:\n",
    "            if type(target) not in (tuple, list):\n",
    "                target = (target, )\n",
    "            loss_inputs += target\n",
    "        return loss_inputs\n",
    "\n",
    "\n",
    "class NPairLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    N-Pair loss\n",
    "    Sohn, Kihyuk. \"Improved Deep Metric Learning with Multi-class N-pair Loss Objective,\" Advances in Neural Information\n",
    "    Processing Systems. 2016.\n",
    "    http://papers.nips.cc/paper/6199-improved-deep-metric-learning-with-multi-class-n-pair-loss-objective\n",
    "    \"\"\"\n",
    "    def __init__(self, l2_reg=0.02, **kwargs):\n",
    "        super(NPairLoss, self).__init__()\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "    def forward(self, embeddings, target):\n",
    "        n_pairs, n_negatives = self.get_n_pairs(target)\n",
    "\n",
    "        if embeddings.is_cuda:\n",
    "            n_pairs = n_pairs.cuda()\n",
    "            n_negatives = n_negatives.cuda()\n",
    "\n",
    "        anchors = embeddings[n_pairs[:, 0]]  # (n, embedding_size)\n",
    "        positives = embeddings[n_pairs[:, 1]]  # (n, embedding_size)\n",
    "        negatives = embeddings[n_negatives]  # (n, n-1, embedding_size)\n",
    "\n",
    "        losses = self.n_pair_loss(anchors, positives, negatives) \\\n",
    "            + self.l2_reg * self.l2_loss(anchors, positives)\n",
    "\n",
    "        return losses\n",
    "\n",
    "    @staticmethod\n",
    "    def get_n_pairs(labels):\n",
    "        \"\"\"\n",
    "        Get index of n-pairs and n-negatives\n",
    "        :param labels: label vector of mini-batch\n",
    "        :return: A tuple of n_pairs (n, 2)\n",
    "                        and n_negatives (n, n-1)\n",
    "        \"\"\"\n",
    "        labels = labels.cpu().data.numpy()\n",
    "        n_pairs = []\n",
    "        for label in set(labels):\n",
    "            label_mask = (labels == label)\n",
    "            label_indices = np.where(label_mask)[0]\n",
    "            if len(label_indices) < 2:\n",
    "                continue\n",
    "            anchor, positive = np.random.choice(label_indices, 2, replace=False)\n",
    "            n_pairs.append([anchor, positive])\n",
    "\n",
    "        n_pairs = np.array(n_pairs)\n",
    "        n_negatives = []\n",
    "        for i in range(len(n_pairs)):\n",
    "            negative = np.concatenate([n_pairs[:i, 1], n_pairs[i + 1:, 1]])\n",
    "            n_negatives.append(negative)\n",
    "\n",
    "        n_negatives = np.array(n_negatives)\n",
    "        return torch.LongTensor(n_pairs), torch.LongTensor(n_negatives)\n",
    "\n",
    "    @staticmethod\n",
    "    def n_pair_loss(anchors, positives, negatives):\n",
    "        \"\"\"\n",
    "        Calculates N-Pair loss\n",
    "        :param anchors: A torch.Tensor, (n, embedding_size)\n",
    "        :param positives: A torch.Tensor, (n, embedding_size)\n",
    "        :param negatives: A torch.Tensor, (n, n-1, embedding_size)\n",
    "        :return: A scalar\n",
    "        \"\"\"\n",
    "        anchors = torch.unsqueeze(anchors, dim=1)  # (n, 1, embedding_size)\n",
    "        positives = torch.unsqueeze(positives, dim=1)  # (n, 1, embedding_size)\n",
    "\n",
    "        x = torch.matmul(anchors, (negatives - positives).transpose(1, 2))  # (n, 1, n-1)\n",
    "        x = torch.sum(torch.exp(x), 2)  # (n, 1)\n",
    "        loss = torch.mean(torch.log(1 + x))\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def l2_loss(anchors, positives):\n",
    "        \"\"\"\n",
    "        Calculates L2 norm regularization loss\n",
    "        :param anchors: A torch.Tensor, (n, embedding_size)\n",
    "        :param positives: A torch.Tensor, (n, embedding_size)\n",
    "        :return: A scalar\n",
    "        \"\"\"\n",
    "        return torch.sum(anchors**2 + positives**2) / anchors.shape[0]\n",
    "\n",
    "\n",
    "class AngularLoss(NPairLoss):\n",
    "    \"\"\"\n",
    "    Angular loss\n",
    "    Wang, Jian. \"Deep Metric Learning with Angular Loss,\" CVPR, 2017\n",
    "    https://arxiv.org/pdf/1708.01682.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, l2_reg=0.02, angle_bound=1., lambda_ang=2, **kwargs):\n",
    "        super(AngularLoss, self).__init__()\n",
    "        self.l2_reg = l2_reg\n",
    "        self.angle_bound = angle_bound\n",
    "        self.lambda_ang = lambda_ang\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, embeddings, target):\n",
    "        n_pairs, n_negatives = self.get_n_pairs(target)\n",
    "\n",
    "        if embeddings.is_cuda:\n",
    "            n_pairs = n_pairs.cuda()\n",
    "            n_negatives = n_negatives.cuda()\n",
    "\n",
    "        anchors = embeddings[n_pairs[:, 0]]  # (n, embedding_size)\n",
    "        positives = embeddings[n_pairs[:, 1]]  # (n, embedding_size)\n",
    "        negatives = embeddings[n_negatives]  # (n, n-1, embedding_size)\n",
    "\n",
    "        losses = self.angular_loss(anchors, positives, negatives, self.angle_bound) \\\n",
    "                 + self.l2_reg * self.l2_loss(anchors, positives)\n",
    "\n",
    "        return losses, 0, 0, 0\n",
    "\n",
    "    @staticmethod\n",
    "    def angular_loss(anchors, positives, negatives, angle_bound=1.):\n",
    "        \"\"\"\n",
    "        Calculates angular loss\n",
    "        :param anchors: A torch.Tensor, (n, embedding_size)\n",
    "        :param positives: A torch.Tensor, (n, embedding_size)\n",
    "        :param negatives: A torch.Tensor, (n, n-1, embedding_size)\n",
    "        :param angle_bound: tan^2 angle\n",
    "        :return: A scalar\n",
    "        \"\"\"\n",
    "        anchors = torch.unsqueeze(anchors, dim=1)  # (n, 1, embedding_size)\n",
    "        positives = torch.unsqueeze(positives, dim=1)  # (n, 1, embedding_size)\n",
    "\n",
    "        x = 4. * angle_bound * torch.matmul((anchors + positives), negatives.transpose(1, 2)) \\\n",
    "            - 2. * (1. + angle_bound) * torch.matmul(anchors, positives.transpose(1, 2))  # (n, 1, n-1)\n",
    "\n",
    "        # Preventing overflow\n",
    "        with torch.no_grad():\n",
    "            t = torch.max(x, dim=2)[0]\n",
    "\n",
    "        x = torch.exp(x - t.unsqueeze(dim=1))\n",
    "        x = torch.log(torch.exp(-t) + torch.sum(x, 2))\n",
    "        loss = torch.mean(t + x)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class NPairAngularLoss(AngularLoss):\n",
    "    \"\"\"\n",
    "    Angular loss\n",
    "    Wang, Jian. \"Deep Metric Learning with Angular Loss,\" CVPR, 2017\n",
    "    https://arxiv.org/pdf/1708.01682.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, l2_reg=0.02, angle_bound=1., lambda_ang=2, **kwargs):\n",
    "        super(NPairAngularLoss, self).__init__()\n",
    "        self.l2_reg = l2_reg\n",
    "        self.angle_bound = angle_bound\n",
    "        self.lambda_ang = lambda_ang\n",
    "\n",
    "    def forward(self, embeddings, target):\n",
    "        n_pairs, n_negatives = self.get_n_pairs(target)\n",
    "\n",
    "        if embeddings.is_cuda:\n",
    "            n_pairs = n_pairs.cuda()\n",
    "            n_negatives = n_negatives.cuda()\n",
    "\n",
    "        anchors = embeddings[n_pairs[:, 0]]  # (n, embedding_size)\n",
    "        positives = embeddings[n_pairs[:, 1]]  # (n, embedding_size)\n",
    "        negatives = embeddings[n_negatives]  # (n, n-1, embedding_size)\n",
    "\n",
    "        losses = self.n_pair_angular_loss(anchors, positives, negatives, self.angle_bound) \\\n",
    "            + self.l2_reg * self.l2_loss(anchors, positives)\n",
    "\n",
    "        return losses, 0, 0, 0\n",
    "\n",
    "    def n_pair_angular_loss(self, anchors, positives, negatives, angle_bound=1.):\n",
    "        \"\"\"\n",
    "        Calculates N-Pair angular loss\n",
    "        :param anchors: A torch.Tensor, (n, embedding_size)\n",
    "        :param positives: A torch.Tensor, (n, embedding_size)\n",
    "        :param negatives: A torch.Tensor, (n, n-1, embedding_size)\n",
    "        :param angle_bound: tan^2 angle\n",
    "        :return: A scalar, n-pair_loss + lambda * angular_loss\n",
    "        \"\"\"\n",
    "        n_pair = self.n_pair_loss(anchors, positives, negatives)\n",
    "        angular = self.angular_loss(anchors, positives, negatives, angle_bound)\n",
    "\n",
    "        return (n_pair + self.lambda_ang * angular) / (1 + self.lambda_ang)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
