{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named torchvision.transforms",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1af408358835>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mImageFolder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named torchvision.transforms"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "from __future__ import absolute_import, print_function\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch.backends import cudnn\n",
    "from evaluations import extract_features, pairwise_distance\n",
    "import os\n",
    "import numpy as np\n",
    "from utils import to_numpy\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from ImageFolder import *\n",
    "from utils import *\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import random\n",
    "from CIFAR100 import CIFAR100\n",
    "import pdb\n",
    "\n",
    "\n",
    "#from tensorboardX import SummaryWriter\n",
    "#writer = SummaryWriter('logs')\n",
    "\n",
    "def displacement(Y1, Y2, embedding_old, sigma):\n",
    "    DY = Y2-Y1\n",
    "    distance = np.sum((np.tile(Y1[None, :, :], [embedding_old.shape[0], 1, 1])-np.tile(\n",
    "        embedding_old[:, None, :], [1, Y1.shape[0], 1]))**2, axis=2)\n",
    "    W = np.exp(-distance/(2*sigma ** 2))  # +1e-5\n",
    "    W_norm = W/np.tile(np.sum(W, axis=1)[:, None], [1, W.shape[1]])\n",
    "    displacement = np.sum(np.tile(W_norm[:, :, None], [\n",
    "                          1, 1, DY.shape[1]])*np.tile(DY[None, :, :], [W.shape[0], 1, 1]), axis=1)\n",
    "    return displacement\n",
    "\n",
    "\n",
    "cudnn.benchmark = True\n",
    "parser = argparse.ArgumentParser(description='PyTorch Testing')\n",
    "\n",
    "parser.add_argument('-data', type=str, default='cub')\n",
    "parser.add_argument('-r', type=str, default='model.pkl', metavar='PATH')\n",
    "parser.add_argument(\"-gpu\", type=str, default='0', help='which gpu to choose')\n",
    "parser.add_argument('-seed', default=1993, type=int, metavar='N',\n",
    "                    help='seeds for training process')\n",
    "parser.add_argument(\"-method\", type=str, default='no', help='Choose FT or SC')\n",
    "parser.add_argument('-mapping_test', help='Print more data',\n",
    "                    action='store_true')\n",
    "parser.add_argument('-sigma_test', default=0, type=float, help='sigma_test')\n",
    "parser.add_argument('-real_mean', help='Print more data', action='store_true')\n",
    "parser.add_argument('-epochs', default=600, type=int,\n",
    "                    metavar='N', help='epochs for training process')\n",
    "parser.add_argument('-exp', type=str, default='exp1',\n",
    "                    help=\"learning rate of new parameters\")\n",
    "parser.add_argument('-task', default=1, type=int, help='task')\n",
    "parser.add_argument('-base', default=50, type=int, help='task')\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "cudnn.benchmark = True\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "models = []\n",
    "\n",
    "for i in os.listdir(args.r):\n",
    "    if i.endswith(\"%d_model.pkl\" % (args.epochs-1)):   # 500_model.pkl\n",
    "        models.append(os.path.join(args.r, i))\n",
    "\n",
    "models.sort()\n",
    "if args.task > 10:\n",
    "    models.append(models[1])\n",
    "    del models[1]\n",
    "\n",
    "if args.data == 'cub':\n",
    "    mean_values = [0.485, 0.456, 0.406]\n",
    "    std_values = [0.229, 0.224, 0.225]\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean_values,\n",
    "                             std=std_values)\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean_values,\n",
    "                             std=std_values),\n",
    "    ])\n",
    "    root = 'DataSet/CUB_200_2011'\n",
    "    traindir = os.path.join(root, 'train')\n",
    "    testdir = os.path.join(root, 'test')\n",
    "\n",
    "    num_classes = 200\n",
    "\n",
    "if args.data == 'car':\n",
    "    mean_values = [0.485, 0.456, 0.406]\n",
    "    std_values = [0.229, 0.224, 0.225]\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean_values,\n",
    "                             std=std_values)\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean_values,\n",
    "                             std=std_values),\n",
    "    ])\n",
    "    root = 'DataSet/Car196'\n",
    "    traindir = os.path.join(root, 'train')\n",
    "    testdir = os.path.join(root, 'test')\n",
    "\n",
    "    num_classes = 196\n",
    "\n",
    "\n",
    "if args.data == 'flower':\n",
    "    mean_values = [0.485, 0.456, 0.406]\n",
    "    std_values = [0.229, 0.224, 0.225]\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean_values,\n",
    "                             std=std_values)\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean_values,\n",
    "                             std=std_values),\n",
    "    ])\n",
    "    root = 'DataSet/flowers'\n",
    "    traindir = os.path.join(root, 'train')\n",
    "    testdir = os.path.join(root, 'test')\n",
    "    num_classes = 102\n",
    "\n",
    "if args.data == \"cifar100\":\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), ])\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    root = 'DataSet'\n",
    "    traindir = root + '/cifar'\n",
    "    testdir = root + '/cifar'\n",
    "    num_classes = 100\n",
    "\n",
    "if args.data == 'imagenet_sub' or args.data == 'imagenet_full':\n",
    "    mean_values = [0.485, 0.456, 0.406]\n",
    "    std_values = [0.229, 0.224, 0.225]\n",
    "    transform_train = transforms.Compose([\n",
    "        # transforms.Resize(256),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean_values,\n",
    "                             std=std_values)\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        # transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean_values,\n",
    "                             std=std_values)\n",
    "    ])\n",
    "    root = '/datatmp/datasets/ILSVRC12_256'\n",
    "    traindir = os.path.join(root, 'train')\n",
    "    testdir = os.path.join(root, 'val')\n",
    "    num_classes = 100\n",
    "\n",
    "\n",
    "num_task = args.task\n",
    "num_class_per_task = (num_classes-args.base)/(num_task-1)\n",
    "np.random.seed(args.seed)\n",
    "random_perm = np.random.permutation(num_classes)\n",
    "\n",
    "\n",
    "print('Test starting -->\\t')\n",
    "\n",
    "class_mean = []\n",
    "class_std = []\n",
    "class_label = []\n",
    "class_mean_mapping = []\n",
    "\n",
    "for task_id in range(num_task):\n",
    "\n",
    "    index = random_perm[:args.base+task_id*num_class_per_task]\n",
    "    if task_id == 0:\n",
    "        index_train = random_perm[:args.base]\n",
    "    else:\n",
    "        index_train = random_perm[args.base +\n",
    "                                  (task_id-1)*num_class_per_task:args.base+task_id*num_class_per_task]\n",
    "\n",
    "    if args.data == 'cifar100':\n",
    "        trainfolder = CIFAR100(root=traindir, train=True, download=True,\n",
    "                               transform=transform_train, index=index_train)\n",
    "        testfolder = CIFAR100(root=traindir, train=False,\n",
    "                              download=True, transform=transform_test, index=index)\n",
    "    else:\n",
    "        trainfolder = ImageFolder(traindir, transform_train, index=index_train)\n",
    "        testfolder = ImageFolder(testdir, transform_test, index=index)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        trainfolder, batch_size=128, shuffle=False, drop_last=False)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        testfolder, batch_size=128, shuffle=False, drop_last=False)\n",
    "    print('Test %d\\t' % task_id)\n",
    "\n",
    "    model = torch.load(models[task_id])\n",
    "\n",
    "    train_embeddings_cl, train_labels_cl = extract_features(\n",
    "        model, train_loader, print_freq=32, metric=None)\n",
    "    val_embeddings_cl, val_labels_cl = extract_features(\n",
    "        model, test_loader, print_freq=32, metric=None)\n",
    "\n",
    "    # Test for each task\n",
    "    for i in index_train:\n",
    "        ind_cl = np.where(i == train_labels_cl)[0]\n",
    "        embeddings_tmp = train_embeddings_cl[ind_cl]\n",
    "        class_label.append(i)\n",
    "        class_mean.append(np.mean(embeddings_tmp, axis=0))\n",
    "\n",
    "    if task_id > 0 and args.mapping_test:\n",
    "        model_old = torch.load(models[task_id-1])\n",
    "        train_embeddings_cl_old, train_labels_cl_old = extract_features(\n",
    "            model_old, train_loader, print_freq=32, metric=None)\n",
    "\n",
    "        MU = np.asarray(class_mean[:args.base+(task_id-1)*num_class_per_task])\n",
    "        gap = displacement(train_embeddings_cl_old,\n",
    "                           train_embeddings_cl, MU, args.sigma_test)\n",
    "        MU += gap\n",
    "        class_mean[:args.base+(task_id-1)*num_class_per_task] = MU\n",
    "\n",
    "    embedding_mean_old = []\n",
    "    embedding_std_old = []\n",
    "    gt_all = []\n",
    "    estimate_all = []\n",
    "\n",
    "    acc_ave = 0\n",
    "    for k in range(task_id+1):\n",
    "        if k == 0:\n",
    "            tmp = random_perm[:args.base]\n",
    "        else:\n",
    "            tmp = random_perm[args.base +\n",
    "                              (k-1)*num_class_per_task:args.base+k*num_class_per_task]\n",
    "        gt = np.isin(val_labels_cl, tmp)\n",
    "\n",
    "        pairwise_distance = euclidean_distances(\n",
    "            val_embeddings_cl, np.asarray(class_mean))\n",
    "        estimate = np.argmin(pairwise_distance, axis=1)\n",
    "        estimate_label = [index[j] for j in estimate]\n",
    "        estimate_tmp = np.asarray(estimate_label)[gt]\n",
    "        if task_id == num_task-1:\n",
    "            if estimate_all == []:\n",
    "                estimate_all = estimate_tmp\n",
    "                gt_all = val_labels_cl[gt]\n",
    "            else:\n",
    "                estimate_all = np.hstack((estimate_all, estimate_tmp))\n",
    "                gt_all = np.hstack((gt_all, val_labels_cl[gt]))\n",
    "\n",
    "        acc = np.sum(estimate_tmp ==\n",
    "                     val_labels_cl[gt])/float(len(estimate_tmp))\n",
    "        if k == 0:\n",
    "            acc_ave += acc*(float(args.base) /\n",
    "                            (args.base+task_id*num_class_per_task))\n",
    "        else:\n",
    "            acc_ave += acc*(float(num_class_per_task) /\n",
    "                            (args.base+task_id*num_class_per_task))\n",
    "        print(\"Accuracy of Model %d on Task %d is %.3f\" % (task_id, k, acc))\n",
    "    print(\"Weighted Accuracy of Model %d is %.3f\" % (task_id, acc_ave))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
