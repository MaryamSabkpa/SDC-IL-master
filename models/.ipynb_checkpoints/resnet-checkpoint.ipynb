{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import init\n",
    "import torchvision\n",
    "import pdb\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18','resnet18_cifar', 'resnet32','resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152']\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class resnet_cifar(nn.Module):\n",
    "\n",
    "    def __init__(self, block, num_blocks,  pretrained=False, cut_at_pooling=False,\n",
    "                 Embed_dim=0, norm=True, dropout=0, num_classes=0):\n",
    "        super(resnet_cifar, self).__init__()\n",
    "        self.Embed_dim = Embed_dim  #lu adds\n",
    "        self.pretrained = pretrained\n",
    "        self.in_planes = 16\n",
    "        self.cut_at_pooling = cut_at_pooling\n",
    " \n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.base_feature = nn.Sequential(self.conv1, self.bn1, self.relu, self.layer1, self.layer2, self.layer3)\n",
    "        self.base = nn.Sequential()\n",
    "        self.base.fc = nn.Linear(64, 1000)\n",
    "        \n",
    "\n",
    "        if not self.cut_at_pooling:\n",
    "            self.num_features = Embed_dim\n",
    "            self.norm = norm\n",
    "            self.dropout = dropout\n",
    "            self.has_embedding = Embed_dim > 0\n",
    "            self.num_classes = num_classes\n",
    "\n",
    "            # Append new layers\n",
    "            if self.has_embedding:\n",
    "                self.base.fc = nn.Linear(64, self.num_features)\n",
    "                init.kaiming_normal_(self.base.fc.weight, mode='fan_out')\n",
    "                init.constant_(self.base.fc.bias, 0)\n",
    "               \n",
    "            self.num_features = 64\n",
    "            if self.dropout > 0:\n",
    "                self.drop = nn.Dropout(self.dropout)\n",
    "            if self.num_classes > 0:\n",
    "                self.classifier = nn.Linear(self.num_features, self.num_classes)\n",
    "                init.normal(self.classifier.weight, std=0.001)\n",
    "                init.constant_(self.classifier.bias, 0)\n",
    "\n",
    "        self.Embed = Embedding(64, 1)\n",
    "        if self.Embed_dim == 0: \n",
    "            pass\n",
    "        else:\n",
    "            self.Embed = Embedding(64, self.Embed_dim)\n",
    "\n",
    "        if not self.pretrained:\n",
    "            self.reset_params()\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.base_feature(x)\n",
    "        if self.cut_at_pooling:\n",
    "            return x\n",
    "        \n",
    "        x = F.avg_pool2d(x, x.size()[2:])\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_feat = x\n",
    "\n",
    "        if self.has_embedding:\n",
    "            x = self.base.fc(x)\n",
    "        if self.norm:\n",
    "            x = F.normalize(x)\n",
    "        elif self.has_embedding:\n",
    "            x = F.relu(x)\n",
    "        if self.dropout > 0:\n",
    "            x = self.drop(x)\n",
    "        if self.num_classes > 0:\n",
    "            x = F.relu(x)\n",
    "            x = self.classifier(x)\n",
    "        return x_feat, x\n",
    "    \n",
    "    def inference(self, x):\n",
    "        x = self.base_feature(x)\n",
    "        if self.cut_at_pooling:\n",
    "            return x\n",
    "        x = F.avg_pool2d(x, x.size()[2:])\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        if self.has_embedding:\n",
    "            x = self.base.fc(x)\n",
    "        if self.norm:\n",
    "            x = F.normalize(x)\n",
    "        elif self.has_embedding:\n",
    "            x = F.relu(x)\n",
    "        if self.dropout > 0:\n",
    "            x = self.drop(x)\n",
    "        \n",
    "        if self.num_classes > 0:\n",
    "            x = F.relu(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "    def forward_without_norm(self, x):\n",
    "        x = self.base_feature(x)\n",
    "        if self.cut_at_pooling:\n",
    "            return x\n",
    "        x = F.avg_pool2d(x, x.size()[2:])\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        if self.has_embedding:\n",
    "            x = self.base.fc(x)\n",
    "        if self.num_classes > 0:\n",
    "            x = F.relu(x)\n",
    "            x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def extract_feat(self, x):\n",
    "        x = self.base_feature(x)\n",
    "        if self.cut_at_pooling:\n",
    "            return x\n",
    "        x = F.avg_pool2d(x, x.size()[2:])\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "    def reset_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    __factory = {\n",
    "        18: torchvision.models.resnet18,\n",
    "        34: torchvision.models.resnet34,\n",
    "        50: torchvision.models.resnet50,\n",
    "        101: torchvision.models.resnet101,\n",
    "        152: torchvision.models.resnet152,\n",
    "    }\n",
    "\n",
    "    def __init__(self, depth, pretrained=True, cut_at_pooling=False,\n",
    "                 Embed_dim=0, norm=True, dropout=0, num_classes=0):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.Embed_dim = Embed_dim  #lu adds\n",
    "        self.depth = depth\n",
    "        self.pretrained = pretrained\n",
    "        self.cut_at_pooling = cut_at_pooling\n",
    "\n",
    "        # Construct base (pretrained) resnet\n",
    "        if depth not in ResNet.__factory:\n",
    "            raise KeyError(\"Unsupported depth:\", depth)\n",
    "        self.base = ResNet.__factory[depth](pretrained=pretrained)\n",
    "\n",
    "        if not self.cut_at_pooling:\n",
    "            self.num_features = Embed_dim\n",
    "            self.norm = norm\n",
    "            self.dropout = dropout\n",
    "            self.has_embedding = Embed_dim > 0\n",
    "            self.num_classes = num_classes\n",
    "\n",
    "            out_planes = self.base.fc.in_features\n",
    "\n",
    "            # Append new layers\n",
    "            if self.has_embedding:\n",
    "                self.base.fc = nn.Linear(out_planes, self.num_features)\n",
    "                init.kaiming_normal_(self.base.fc.weight, mode='fan_out')\n",
    "                init.constant_(self.base.fc.bias, 0)\n",
    "                # Change the num_features to CNN output channels\n",
    "            self.num_features = out_planes\n",
    "            if self.dropout > 0:\n",
    "                self.drop = nn.Dropout(self.dropout)\n",
    "            if self.num_classes > 0:\n",
    "                self.classifier = nn.Linear(self.num_features, self.num_classes)\n",
    "                init.normal(self.classifier.weight, std=0.001)\n",
    "                init.constant_(self.classifier.bias, 0)\n",
    "\n",
    "        self.Embed = Embedding(1024, 1)\n",
    "        if self.Embed_dim == 0:  \n",
    "            pass\n",
    "        else:\n",
    "            self.Embed = Embedding(1024, self.Embed_dim)\n",
    "\n",
    "        if not self.pretrained:\n",
    "            self.reset_params()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for name, module in self.base._modules.items():\n",
    "            if name == 'avgpool':\n",
    "                break\n",
    "            x = module(x)\n",
    "\n",
    "        if self.cut_at_pooling:\n",
    "            return x\n",
    "        x = F.avg_pool2d(x, x.size()[2:])\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_feat = x\n",
    "\n",
    "        if self.has_embedding:\n",
    "            x = self.base.fc(x)\n",
    "        if self.norm:\n",
    "            x = F.normalize(x)\n",
    "        elif self.has_embedding:\n",
    "            x = F.relu(x)\n",
    "        if self.dropout > 0:\n",
    "            x = self.drop(x)\n",
    "        if self.num_classes > 0:\n",
    "            x = F.relu(x)\n",
    "            x = self.classifier(x)\n",
    "        return x_feat, x\n",
    "    \n",
    "    def inference(self, x):\n",
    "        for name, module in self.base._modules.items():\n",
    "            if name == 'avgpool':\n",
    "                break\n",
    "            x = module(x)\n",
    "\n",
    "        if self.cut_at_pooling:\n",
    "            return x\n",
    "        x = F.avg_pool2d(x, x.size()[2:])\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        if self.has_embedding:\n",
    "            x = self.base.fc(x)\n",
    "        if self.norm:\n",
    "            x = F.normalize(x)\n",
    "        elif self.has_embedding:\n",
    "            x = F.relu(x)\n",
    "        if self.dropout > 0:\n",
    "            x = self.drop(x)\n",
    "        \n",
    "        if self.num_classes > 0:\n",
    "            x = F.relu(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "    def forward_without_norm(self, x):\n",
    "        for name, module in self.base._modules.items():\n",
    "            if name == 'avgpool':\n",
    "                break\n",
    "            x = module(x)\n",
    "\n",
    "        if self.cut_at_pooling:\n",
    "            return x\n",
    "        x = F.avg_pool2d(x, x.size()[2:])\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        if self.has_embedding:\n",
    "            x = self.base.fc(x)\n",
    "        if self.num_classes > 0:\n",
    "            x = F.relu(x)\n",
    "            x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def extract_feat(self, x):\n",
    "        for name, module in self.base._modules.items():\n",
    "            if name == 'avgpool':\n",
    "                break\n",
    "            x = module(x)\n",
    "\n",
    "        if self.cut_at_pooling:\n",
    "            return x\n",
    "        x = F.avg_pool2d(x, x.size()[2:])\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "    def reset_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "class Embedding(nn.Module): \n",
    "    def __init__(self, in_dim, out_dim, dropout=None, normalized=True):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_dim, eps=0.001)\n",
    "        self.linear = nn.Linear(in_features=in_dim, out_features=out_dim)\n",
    "        self.dropout = dropout\n",
    "        self.normalized = normalized\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        if self.dropout is not None:\n",
    "            x = nn.Dropout(p=self.dropout)(x, inplace=True)\n",
    "        x = self.linear(x)\n",
    "        if self.normalized:\n",
    "            norm = x.norm(dim=1, p=2, keepdim=True)\n",
    "            x = x.div(norm.expand_as(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet18(**kwargs):\n",
    "    return ResNet(18, **kwargs)\n",
    "\n",
    "def resnet32(**kwargs):\n",
    "    return resnet_cifar(BasicBlock, [5, 5, 5], **kwargs)\n",
    "\n",
    "def resnet34(**kwargs):\n",
    "    return ResNet(34, **kwargs)\n",
    "\n",
    "def resnet50(**kwargs):\n",
    "    return ResNet(50, **kwargs)\n",
    "\n",
    "\n",
    "def resnet101(**kwargs):\n",
    "    return ResNet(101, **kwargs)\n",
    "\n",
    "\n",
    "def resnet152(**kwargs):\n",
    "    return ResNet(152, **kwargs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
